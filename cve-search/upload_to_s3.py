#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Shard all vendor–product CVE lists into N gzip’d NDJSON parts under a single
date folder, and upload each part to S3 so downstream Glue can process efficiently.

Usage:
  INGESTION_DATE=2025-03-30 python3 upload_to_s3.py
  # or let it default to today's UTC date
"""
import os
import sys
import json
import gzip
import subprocess
import signal
from hashlib import md5
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from pymongo import MongoClient

# ──────────────────────────────────────────────────────────────────────────────
# Configuration
# ──────────────────────────────────────────────────────────────────────────────
S3_BUCKET      = os.getenv("S3_BUCKET", "cve-ingestion")
S3_PREFIX_BASE = os.getenv("S3_PREFIX", "cve_json/")  
AWS_CLI        = os.getenv("AWS_CLI", "aws")

MONGO_URI = os.getenv("MONGO_URI", "mongodb://mongo:27017/")
DB_NAME   = os.getenv("DB_NAME", "cvedb")

# total compressed size ~7 GB → ~128 MB per shard → 56 shards
N_SHARDS       = int(os.getenv("N_SHARDS", "56"))
MAX_CONCURRENCY = int(os.getenv("MAX_CONCURRENCY", "8"))

# ──────────────────────────────────────────────────────────────────────────────
# Prep ingestion date and prefix
# ──────────────────────────────────────────────────────────────────────────────
ingestion_date = os.getenv(
    "INGESTION_DATE",
    datetime.now(timezone.utc).date().isoformat()
)
s3_prefix = f"{S3_PREFIX_BASE.rstrip('/')}/{ingestion_date}/"

# ──────────────────────────────────────────────────────────────────────────────
# Graceful shutdown helpers
# ──────────────────────────────────────────────────────────────────────────────
aws_procs = []
def cleanup_and_exit(code=1):
    for p in aws_procs:
        try:
            p.kill()
        except Exception:
            pass
    sys.exit(code)

signal.signal(signal.SIGINT, lambda *_: cleanup_and_exit(1))
signal.signal(signal.SIGTERM, lambda *_: cleanup_and_exit(1))

# ──────────────────────────────────────────────────────────────────────────────
# Launch AWS CLI subprocess for one shard
# ──────────────────────────────────────────────────────────────────────────────
def start_shard_proc(i):
    part = f"part-{i:04d}.ndjson.gz"
    uri  = f"s3://{S3_BUCKET}/{s3_prefix}{part}"
    cmd  = [
        AWS_CLI, "s3", "cp", "-", uri,
        "--content-type",     "application/x-ndjson",
        "--content-encoding", "gzip"
    ]
    p = subprocess.Popen(cmd, stdin=subprocess.PIPE)
    if not p.stdin:
        raise RuntimeError(f"Cannot open stdin for shard {i}")
    aws_procs.append(p)
    gz = gzip.GzipFile(fileobj=p.stdin, mode="wb")
    return (i, p, gz)

# ──────────────────────────────────────────────────────────────────────────────
# Worker: consume one shard’s gzip writer, then wait for its upload
# ──────────────────────────────────────────────────────────────────────────────
def finalize_shard(shard_tuple):
    i, proc, gz = shard_tuple
    gz.close()
    proc.stdin.close()
    rc = proc.wait()
    if rc != 0:
        raise RuntimeError(f"Shard {i} upload failed (exit {rc})")
    return i

# ──────────────────────────────────────────────────────────────────────────────
# Main
# ──────────────────────────────────────────────────────────────────────────────
def main():
    # 1) Spin up all shard procs & gzip writers
    shards = [start_shard_proc(i) for i in range(N_SHARDS)]

    # 2) MongoDB aggregation: join CVE-List map with NVD map
    client = MongoClient(MONGO_URI)
    coll  = client[DB_NAME].map_vendor_product_cvelistv5
    pipeline = [
        # lookup NVD map docs by _id
        {"$lookup": {
            "from": "map_vendor_product_nvd",
            "localField": "_id",
            "foreignField": "_id",
            "as": "nvd_map"
        }},
        {"$project": {
            "_id": 1,
            "cl_ids": "$cves",
            # nvd_map.cves is an array of arrays; take first
            "nvd_ids": {"$arrayElemAt": ["$nvd_map.cves", 0]}
        }}
    ]
    cursor = coll.aggregate(pipeline, allowDiskUse=True)

    # 3) Stream each vendor–product into its shard
    for mp in cursor:
        vid, pid = mp["_id"]["vendor"], mp["_id"]["product"]
        cl_ids    = mp.get("cl_ids", [])
        nvd_ids   = mp.get("nvd_ids", []) or []

        # build record header
        now = datetime.now(timezone.utc)
        header = {
            "vendor":             vid,
            "product":            pid,
            "ingestionTimestamp": now.isoformat(),
            "ingestionDate":      now.date().isoformat(),
            "fkie_nvd":   None,  # placeholder
            "cvelistv5":  None
        }
        # serialize header up to fkie_nvd
        hjson = json.dumps(header, ensure_ascii=False)
        prefix, suffix = hjson.split('"fkie_nvd": null', 1)
        # prefix ends with ..., "fkie_nvd": , so:
        out = [prefix + '"fkie_nvd": [']

        # stream NVD docs
        first = True
        for doc in client[DB_NAME].cves.find({"id": {"$in": nvd_ids}}, {"_id":0}):
            chunk = json.dumps(doc, ensure_ascii=False)
            out.append((not first and ",") or "" + chunk)
            first = False
        out.append("],")

        # cvelistv5
        out.append('"cvelistv5":[')
        first = True
        for doc in client[DB_NAME].cvelistv5.find(
            {"cveMetadata.cveId": {"$in": cl_ids}}, {"_id": 0}
        ):
            chunk = json.dumps(doc, ensure_ascii=False)
            out.append((not first and ",") or "" + chunk)
            first = False
        out.append("]}\n")

        line = "".join(out).encode("utf-8")

        # pick shard
        idx = int(md5(f"{vid}:{pid}".encode("utf-8")).hexdigest(), 16) % N_SHARDS
        shards[idx][2].write(line)

    # 4) Finalize in a small thread‐pool to limit concurrency
    with ThreadPoolExecutor(max_workers=MAX_CONCURRENCY) as pool:
        futures = [pool.submit(finalize_shard, s) for s in shards]
        for f in as_completed(futures):
            try:
                sid = f.result()
            except Exception as e:
                sys.stderr.write(f"[!] {e}\n")
                cleanup_and_exit(1)

    print(f"✔ Uploaded {N_SHARDS} shards to s3://{S3_BUCKET}/{s3_prefix}")

if __name__ == "__main__":
    main()
