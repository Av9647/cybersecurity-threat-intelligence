#!/usr/bin/env python3
"""
Import the official CVE List V5 into MongoDB, optimizing for
first-run bulk load and on subsequent runs only upserting changed files.
All fields are stored verbatim; normalization happens downstream.
"""

import glob
import json
from pathlib import Path
from git import Repo
from pymongo import MongoClient, ReplaceOne, errors

# --------------------------------------------------------------------------- #
# settings
REPO_URL    = "https://github.com/CVEProject/cvelistV5.git"
LOCAL_DIR   = Path("./cvelistV5")
LAST_SHA    = LOCAL_DIR / ".last_sha"
DB_NAME     = "cvedb"
COLL_NAME   = "cvelistv5"
BATCH_SIZE  = 1000
MONGO_URI   = "mongodb://mongo:27017/"
INDEX_KEY   = "cveMetadata.cveId"
# --------------------------------------------------------------------------- #

# 1) Clone or update the repo
if not LOCAL_DIR.is_dir():
    print("Cloning cvelistV5 (shallow)…")
    repo = Repo.clone_from(REPO_URL, LOCAL_DIR, depth=1)
    old_sha = None
else:
    repo    = Repo(str(LOCAL_DIR))
    old_sha = LAST_SHA.read_text().strip() if LAST_SHA.exists() else None

    print("Fetching latest cvelistV5…")
    repo.remotes.origin.fetch()                  # full fetch so we can diff
    branch = repo.active_branch.name
    repo.git.reset("--hard", f"origin/{branch}")
    print(f" → reset to origin/{branch}")

new_sha = repo.head.commit.hexsha
LAST_SHA.write_text(new_sha)

# 2) Connect to Mongo
client = MongoClient(MONGO_URI)
coll   = client[DB_NAME][COLL_NAME]

# 3) First run?
is_first_run = (coll.estimated_document_count() == 0)

if is_first_run:
    ############################
    # INITIAL BULK LOAD
    ############################
    print("First run: full bulk insert…")
    docs = []
    count = 0
    for path in glob.iglob(str(LOCAL_DIR / "cves" / "**" / "CVE-*.json"), recursive=True):
        try:
            rec = json.load(open(path, "r"))
        except Exception:
            continue

        docs.append(rec)
        count += 1

        if len(docs) >= BATCH_SIZE:
            coll.insert_many(docs, ordered=False)
            docs.clear()
            print(f"{count:,} inserted…", flush=True)

    if docs:
        coll.insert_many(docs, ordered=False)
        print(f"{count:,} final batch inserted.")

    print(f"Creating unique index on '{INDEX_KEY}'…")
    coll.create_index(INDEX_KEY, unique=True)

    print(f"[import_cvelistv5] {count:,} records inserted.")

else:
    ############################
    # INCREMENTAL UPDATES
    ############################
    print("Subsequent run: upserting changed CVEs…")
    try:
        coll.create_index(INDEX_KEY, unique=True)
    except errors.OperationFailure:
        pass

    # find changed JSON paths
    if old_sha:
        diff_files = repo.git.diff("--name-only", old_sha, new_sha).splitlines()
    else:
        diff_files = []

    # filter only CVE JSONs under cves/
    to_process = [
        LOCAL_DIR / f for f in diff_files
        if f.startswith("cves/") and f.endswith(".json")
    ]

    if not to_process:
        print("No CVE JSON changes to process.")
    else:
        ops = []
        count = 0
        for path in to_process:
            try:
                rec    = json.load(open(path, "r"))
                cve_id = rec["cveMetadata"]["cveId"]
            except Exception:
                continue

            ops.append(
                ReplaceOne(
                    {INDEX_KEY: cve_id},
                    rec,
                    upsert=True
                )
            )
            count += 1

            if len(ops) >= BATCH_SIZE:
                coll.bulk_write(ops, ordered=False)
                ops.clear()
                print(f"{count:,} upserted…", flush=True)

        if ops:
            coll.bulk_write(ops, ordered=False)
            print(f"{count:,} final batch upserted.")

        print(f"[import_cvelistv5] {count:,} records upserted.")
