#!/usr/bin/env python3
"""
Diff the two most recent builds of BOTH map_vendor_product_nvd
and map_vendor_product_cvelistv5, then send changed vendor/product
pairs to SQS in batches of 10.
"""
import os
import json
from datetime import datetime, timezone
from pymongo import MongoClient
import boto3
from itertools import islice

# config from env
MONGO_URI   = os.getenv("MONGO_URI", "mongodb://mongo:27017/")
DB_NAME     = os.getenv("DB_NAME",   "cvedb")
QUEUE_URL   = os.getenv("DELTA_QUEUE_URL", "https://sqs.us-east-2.amazonaws.com/692859941232/cve_ingestion_vendor_product_sqs")

# constants
MAP_NVD       = "map_vendor_product_nvd"
MAP_CVELIST   = "map_vendor_product_cvelistv5"
META_COLL     = "map_build_meta"
BATCH_SIZE    = 10   # SQS max batch size

# connect
client = MongoClient(MONGO_URI)
db     = client[DB_NAME]
sqs    = boto3.client("sqs")

def last_two_ts(tag):
    docs = (db[META_COLL]
            .find({"collection": tag})
            .sort("ts", -1)
            .limit(2))
    docs = list(docs)
    if len(docs) < 2:
        raise RuntimeError(f"Need ≥2 builds for '{tag}'")
    return docs[0]["ts"], docs[1]["ts"]

new_nvd_ts, prev_nvd_ts   = last_two_ts("nvd_ts")
new_cl_ts,  prev_cl_ts    = last_two_ts("cvelistv5_ts")

print(f"Diffing NVD map:       {prev_nvd_ts} → {new_nvd_ts}")
print(f"Diffing CVE-List map:  {prev_cl_ts} → {new_cl_ts}")

# build the aggregation pipeline
pipeline = [
    # start with new NVD entries
    {"$match": {"_build_ts": new_nvd_ts}},
    # union in the new CVE-List entries
    {"$unionWith": {
        "coll": MAP_CVELIST,
        "pipeline": [
            {"$match": {"_build_ts": new_cl_ts}}
        ]
    }},
    # keep only docs from either new build
    {"$match": {"_build_ts": {"$in": [new_nvd_ts, new_cl_ts]}}},
    # lookup the *old* NVD cves array
    {"$lookup": {
        "from": MAP_NVD,
        "let": {"id": "$_id"},
        "pipeline": [
            {"$match": {"$expr": {"$and": [
                {"$eq": ["$_id", "$$id"]},
                {"$eq": ["$_build_ts", prev_nvd_ts]}
            ]}}},
            {"$project": {"cves": 1}}
        ],
        "as": "old_nvd"
    }},
    # lookup the *old* CVE-List cves array
    {"$lookup": {
        "from": MAP_CVELIST,
        "let": {"id": "$_id"},
        "pipeline": [
            {"$match": {"$expr": {"$and": [
                {"$eq": ["$_id", "$$id"]},
                {"$eq": ["$_build_ts", prev_cl_ts]}
            ]}}},
            {"$project": {"cves": 1}}
        ],
        "as": "old_cl"
    }},
    # keep only those where EITHER array changed
    {"$match": {"$expr": {
        "$or": [
            # new_nvd.cves \ old_nvd.cves != ∅
            {"$gt": [
                {"$size": {
                    "$setDifference": [
                        "$cves",
                        {"$arrayElemAt": ["$old_nvd.cves", 0]}
                    ]
                }},
                0
            ]},
            # new_cl.cves \ old_cl.cves != ∅
            {"$gt": [
                {"$size": {
                    "$setDifference": [
                        "$cves",
                        {"$arrayElemAt": ["$old_cl.cves", 0]}
                    ]
                }},
                0
            ]}
        ]
    }}},
    # dedupe across NVD & CVE-List (if a pair changed in both)
    {"$group": {"_id": "$_id"}},
    # project vendor & product fields for SQS messages
    {"$project": {
        "_id": 0,
        "vendor": "$_id.vendor",
        "product": "$_id.product"
    }}
]

# helper to batch an iterator into lists of size n
def batched(it, n):
    it = iter(it)
    while True:
        batch = list(islice(it, n))
        if not batch:
            return
        yield batch

# run the aggregation
cursor = db[MAP_NVD].aggregate(pipeline, allowDiskUse=True)

# capture one timestamp for this entire run
run_ts = datetime.now(timezone.utc).isoformat()

sent = 0
for batch in batched(cursor, BATCH_SIZE):
    entries = []
    for doc in batch:
        body = {
            "vendor": doc["vendor"],
            "product": doc["product"],
            "ingestionTimestamp": run_ts
        }
        entries.append({
            "Id":          f"{doc['vendor']}#{doc['product']}",
            "MessageBody": json.dumps(body, ensure_ascii=False)
        })
    # send up to 10 messages at once
    resp = sqs.send_message_batch(QueueUrl=QUEUE_URL, Entries=entries)
    if resp.get("FailedPutCount", 0) > 0:
        failed = [f['Id'] for f in resp['Failed']]
        raise RuntimeError(f"SQS batch send failed for IDs: {failed}")
    sent += len(entries)

print(f"✔ Diff complete; sent {sent} messages to SQS.")
